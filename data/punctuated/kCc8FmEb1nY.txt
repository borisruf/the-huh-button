 Hi everyone! So, by now, you have probably heard of ChatGPT. It has taken the world and the AI Community by storm and it is a system that allows you to interact with an AI and give it text-based tasks. So, for example, we can ask ChatGPT to write us a small haiku about how important it is that people understand AI and then they can use it to improve the world and make it more prosperous. 

So, when we run this AI: 
"Knowledge brings prosperity for all to see, Embrace its power." 

Okay, not bad. And so, you... Could see that ChatGPT went from left to right and generated all these words, sort of sequentially. Now, I asked it already the exact same prompt a little bit earlier and it generated a slightly different outcome; "AI is power to grow, ignorance holds us back, learn, Prosperity weights." So, uh, pretty good in both cases and slightly different. So you can see that ChatGPT is a probabilistic system and for any one prompt, it can give us multiple answers sort of replying to it. Now, this is just one example of a prompt people have come up. With many, many examples, there are entire websites that index interactions with charge EBT. So many of them are quite humorous: "Explain HTML to me like I'm a dog", "Write release notes for Chess 2", "Write a note about Elon Musk buying on Twitter", and so on. 

So, as an example, please write a breaking news article about a leaf falling from a tree. Uh, and in a shocking turn of events, a leaf has fallen from a tree in the local park. Witnesses report that the leaf, which was previously attached to a branch of a tree, detached. Itself, and fell to the ground, very dramatic. So, you can see that this is a pretty remarkable system, and it is what we call a language model because it models the sequence of words, or characters, or tokens more generally. And, it knows how sort of words follow each other in English language. And so, from its perspective, what it is doing is, it is completing the sequence. So, I give it the start of a sequence, and it completes the sequence with the outcome. And so, it's a language model in that sense. Now, I would like "To focus on the 'under the hood' of, um, 'under the hood' components of what makes Chat GPT work; so, what is the neural network under the hood that models the sequence of these words? This comes from a paper called 'Attention is All You Need' in 2017, a landmark paper, a landmark paper in AI that produced and proposed the Transformer architecture. So, GPT is short for 'Generally Generative Pre-trained Transformer.' So, Transformer is the neural nut that actually does all the heavy lifting 'under the hood.' It comes from this paper in 2017. Now, if you read..." "This paper reads like a pretty random machine translation paper, and that's because I think the authors didn't fully anticipate the impact that the Transformer would have on the field. This architecture that they produced in the context of machine translation, in their case, actually ended up taking over the rest of AI in the next five years. After this architecture, with minor changes, was copy-pasted into a huge amount of applications in AI in more recent years. That includes at the core of chat GPT. Now, we are not going to what I'd..." What I'd like to do now is, I'd like to build out something like ChatGPT. But we're not going to be able to, of course, reproduce ChatGPT. This is a very serious, production-grade system. It is trained on a good chunk of the internet, and then there's a lot of pre-training and fine-tuning stages to it. So it's very complicated. What I'd like to focus on is just to train a Transformer-based language model, and in our case, it's going to be a character-level language model. I still think that is very educational with respect to how... These systems work, so I don't want to train on the chunk of Internet. We need a smaller data set. In this case, I propose that we work with my favorite toy data set. It's called Tiny Shakespeare. What it is, is basically it's a concatenation of all of the works of Shakespeare, in my understanding. And, so this is all of Shakespeare in a single file. This file is about one megabyte, and it's just all of Shakespeare. And what we are going to do now is, we're going to basically model how these characters follow each other. So, for example, given a chunk of these characters, like this, are given some context of characters in the past, the Transformer neural network will look at the characters that I've highlighted and is going to predict that 'g' is likely to come next in the sequence. And it's going to do that because we're going to train that Transformer on Shakespeare, and it's just going to try to produce character sequences that look like this. And in that process, it is going to model all the patterns inside this data. So, once we've trained the system, I'd just like... "To give you a preview, we can generate infinite Shakespeare and of course, it's a fake thing that looks kind of like Shakespeare. Um, apologies for there's some junk that I'm not able to resolve in here, but um, you can see how this is going character by character and it's kind of like predicting Shakespeare-like language. So verily, my Lord, the sights have left me again. The king coming with my curses, with precious pale, and then Tronio says something else, etc. And this is just coming out of the Transformer in a very similar manner as...
