The speaker is discussing how a Transformer neural network, which forms the basis of AI language models like ChatGPT, works. After being trained on a sequence of characters from a text (in this case, a compilation of all of Shakespeare's works), the Transformer neural network uses past characters, or context, to predict the next character in a sequence. The speaker uses the example of providing the AI with a string of characters, and then the AI predicting that the next character in the sequence would be 'g.' This prediction capability lies at the heart of language models' ability to generate coherent and diverse responses to text prompts.