In the video, Andrej explains that they intend to build a language model on a character level, similar to how ChatGPT processes text. He explains that while they cannot reproduce the exact complexity of ChatGPT, as it is a production-grade system that has been trained on a large portion of the internet, they will work with a smaller dataset referred to as 'Tiny Shakespeare', which is a concatenation of all Shakespeare's works. The aim is to train the Transformer neural network to predict the next character in a given sequence, modeling language patterns within the works of Shakespeare. This way, he will demonstrate how a Transformer can generate text that resembles Shakespeare's style. Furthermore, he mentions that he's already written code for training Transformers, which is available in a GitHub repository, and his goal is to help viewers understand how ChatGPT operates behind the scenes. For this exercise, some knowledge of Python, statistics, and calculus is required.