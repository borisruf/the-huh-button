The explanation refers to how artificial intelligence processes language. In this case, it is referring to a process known as tokenization, where raw text is converted into a sequence of integers, with each integer representing a unique character or group of characters (tokens). These tokens enable the AI to analyze, understand, and generate text.

The reference to "65 possible characters or tokens" likely refers to an AI model that operates on a character-level, where each token corresponds to a single character (i.e., the English alphabet upper case and lower case characters, digits, and punctuation).

In contrast, having "50,000 tokens" suggests using a sub-word tokenization scheme (like sentence-piece or Byte-Pair Encoding (BPE)) that breaks text into meaningful chunks (which could be words, word-parts, or even combination of words), not just individual characters. This latter approach is more advanced and allows for greater linguistic and contextual understanding, as it can capture more complex linguistic elements in a token, hence a better and robust language model. 

In GPT-2 model, the use of 50,000 tokens allows the model to recognize and generate a broader range of text inputs and outputs, enhancing its understanding of, and ability to mimic, natural language.