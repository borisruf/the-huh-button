The speaker essentially says that they are going to train a Transformer-based language model using the "Tiny Shakespeare" dataset - a dataset that encompasses all of Shakespeare's works. The speaker states that the Transformer will analyse the sequence of characters - for example, understanding that a 'g' is likely to follow a sequence of characters like 'kin'. In applying Transformer to the dataset, they hope to model all the patterns in the text to generate an infinite amount of 'Shakespeare-like' text. The speaker speaks about a simple implementation of a Transformer model on their Github page, but in this discussion, they plan to write this code from scratch and train it on the Tiny Shakespeare dataset. The end goal of this work is to assist viewers in understanding how Chat GPT works.