The sentence "But there's many other schemas that people have come up with in practice" refers to alternative ways or methods that individuals have developed to encode raw text into a sequence of integers, or tokenize it, as part of the process of training language models like ChatGPT. These techniques aren't limited to character-level translation; they might include word-level, subword-level tokenization, or more complex techniques such as BPE (Byte Pair Encoding). The goal is to find a balance between simplicity, tokenizer size, and ability to capture language structures.