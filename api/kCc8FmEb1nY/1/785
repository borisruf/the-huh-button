The speaker hyped up the current topic, ChatGPT, by highlighting its ability to generate text based on a given prompt. He demonstrated several humorous examples of prompts and their corresponding outputs. He then explained that ChatGPT is a probabilistic system, meaning it can generate different answers to the same prompt because it is based on a language model, a tool that predicts the next item in a sequence based on the previous items.

Subsequently, he revealed the foundation of ChatGPT, the Transformer architecture, which is based on a paper called "Attention is All You Need." The system uses a generatively pre-trained Transformer, which was revolutionary in the field of artificial intelligence, to predict the sequence of words.

The speaker then expressed his intent to demonstrate the process by training a language model on his preferred text dataset, "Tiny Shakespeare." While acknowledging that their machine may not be as complex as ChatGPT, the speaker clarified his objective is to help people understand how language models, particularly transformers, work.

He then walked through the initial stages of preparing the "Tiny Shakespeare" text for the language model, explaining the tokenization process, which converts raw text to integers. He underlined that in practice complex models such as GPT-2 use sub-word-tokenizers that produce shorter, more manageable sequences. However, he decided to use a simpler, character-level tokenizer for the current demonstration, emphasizing that this choice would lead to very long sequences.

For the remainder of the discussion, he would work with the entire Shakespeare text represented as an extremely long sequence of integers.