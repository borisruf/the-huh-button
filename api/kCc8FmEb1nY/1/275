The speaker is proposing to demonstrate a simplified version of how ChatGPT works using the simpler 'Tiny Shakespeare' data set. They will train a Transformer-based language model, which is the type of neural network underpinning the ChatGPT system. It is designed to predict, given a sequence of characters, what character is likely to come next. This will give the audience a preview of how a language model, similar to ChatGPT, learns to generate text by analyzing patterns in the dataset, in this case, the works of Shakespeare.