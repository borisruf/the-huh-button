In the video, Andrej explains that they will build a simple version of ChatGPT using the Transformer neural network architecture. This architecture was proposed in a 2017 research paper called "Attention is All You Need". Andrej points out that, while they can't completely reproduce ChatGPT due to its complexity and extensive training process, they aim to train a Transformer-based language model on a much smaller dataset for educational purposes. He introduces 'Tiny Shakespeare', a complete compilation of all Shakespeare's works, as their chosen dataset. He emphasizes that the trained system will generate sequences of characters that adapt to patterns in the Shakespeare data. Andrej also mentions a GitHub repository called 'nano GPT' where he's written code to train Transformers on any given text and reproduces the performance of GPT-2, a smaller, earlier version of OpenAI's GPT. Finally, he plans to write this repository from scratch within a Jupyter notebook on Google Colab, which viewers can access and follow along with.