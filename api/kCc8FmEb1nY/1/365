The speaker is discussing the creation of a simple, albeit lesser version of a language model like ChatGPT. The language model they will create is a transformer-based language model trained on a dataset called 'Tiny Shakespeare', which contains all of Shakespeare's works. This model is designed to predict the next character in a sequence based on the characters that come before it. The speaker also mentions a GitHub repository called 'nano GPT' - a simple implementation for training Transformers on any provided text. This repository consists of two files: one that defines the transformer or GPT model, and another that trains it on a chosen text dataset.