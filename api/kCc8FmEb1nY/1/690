Andrej plans to build a simple version of ChatGPT, a large-scale, Transformer-based language model. Despite not having the resources or ability to replicate the complexity of OpenAI's ChatGPT precisely, he aims to educate viewers about how such systems work by using a smaller dataset called 'Tiny Shakespeare'. Andrej intends to train the Transformer to predict the next character in a sequence based on previous characters. This would model the patterns within the data and potentially generate Shakespeare-like text. In preparation, he has written code and saved it in a GitHub repository named 'nano GPT'. It is located in his repository and has two files that define the Transformer and allow it to be trained on different text datasets. Andrej's ultimate goal is to ensure his audience comprehends how Chat GPT operates.