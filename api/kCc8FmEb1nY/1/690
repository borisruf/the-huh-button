In the training of AI systems like GPT, turning sequences of characters or words into corresponding integers is known as tokenization. This process simplifies the task for the neural network, making it useful for language models or text-based AI models. A simple form of tokenization is character-level tokenization, where every unique character is mapped with a unique integer. However, there are more sophisticated methods as well, such as the "sentence piece" and "byte pair encoding" methods, which map at a sub-word unit level, splitting words into commonly-occurring pieces. These methods result in longer code books (more unique tokens), but shorter sequences (fewer tokens to represent the same text), which can be desirable depending on the application.