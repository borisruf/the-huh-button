The speaker is explaining a tokenization method used in training transformers for language modelling in the context of AI. They describe starting with all the characters used in a dataset of text, and using these as the vocabulary. Each individual character is then tokenized into integers, creating a mapping that can be used to encode and decode text, converting it back and forth between a string and a sequence of integers. This method, albeit simpler, increases the sequence length compared to other methods such as sub-word encodings. Despite this, the speaker chooses to stick with this level of explanation for the purpose of the lecture, due to its simplicity.