The neural network under the hood of ChatGPT that models the sequence of words is called a Transformer model, specifically a GPT (Generative Pretrained Transformer) model developed by OpenAI. At its core, it uses a mechanism called "attention" to understand the context and relationship between words in a sequence. This model has been trained on vast amounts of text data, allowing it to predict the next word in a sequence based on the previous words. The ability to generate unique and varied responses comes from the model's probabilistic nature - given the same prompt, it generates slightly different outcomes each time, which explains why the chatbot's responses vary even when the same question is asked. The 'transformer architecture' in itself helps in significantly reducing the amount of computations needed by simplifying the way words relate to each other in a sentence. Overall, these networks help in understanding the context and semantics of a language and produce coherent sentences.