This part of the lecture discusses how to build and train a Transformer-based language model using 'Tiny Shakespeare' dataset. It walks through the process of reading the data, tokenizing the plain texts into sequences of integers, and creating a tiny neural network. It also highlights that the model will be trained according to the data, predicting the next character in a sequence based on the previous ones. As a result, the substrings it produces in prediction would mimic the patterns in the given text data. The model they're about to build is a simplified version of GPT (Generative Pre-trained Transformer), one of the most powerful language models by OpenAI.