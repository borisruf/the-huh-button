Andrej explains in the video that he aims to build a transformer-based language model using a significantly smaller dataset than what is used in ChatGPT, the "Tiny Shakespeare" dataset. This dataset is simply a compilation of all of Shakespeare's works. The goal of the transformer-based language model that he plans to build, is to predict what character comes next in a sequence based on the characters that preceded it, effectively modeling patterns within the data. This language model, once trained, would be able to generate an infinite string of Shakespeare-like text. Further, Andrej also mentions that he has created a GitHub repository called nano GPT, that includes simple implementation codes for training Transformers. Summing up, Andrej's goal in the video is to facilitate understanding of how language model such as Chat GPT works at a basic level.