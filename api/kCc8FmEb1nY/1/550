Tokenization is the process of converting text into a sequence of integers. This is done according to a specific vocabulary of possible elements. Each integer or "token" represents a character, word, or sub-word in the text. The tokenization process ensures that the text is in a suitable format for processing in natural language processing (NLP) tasks, such as the training of language models like ChatGPT. Given a raw text document, the tokenizer will convert this into a series of tokens that the model can understand and generate outputs from.