 "Sentence piece" is a tokenization scheme developed by Google. Unlike character-level tokenization, where each character is translated to an integer, or word-level tokenization, where each word is translated to an integer, sentence piece tokenization breaks input text down into sub-words or smaller pieces. This can be very useful for languages where words can be broken down into smaller meaningful parts. For the context of AI models such as ChatGPT, these sub-words or tokens are not word level, they're kind of like sub-word chunk level which can provide more flexibility and granularity in language processing tasks.