The neural network under the hood of ChatGPT that models the sequence of words is a transformer-based model. The architecture is designed to start with a given sequence of words and predict the next word in the sequence. It has been trained on a vast amount of internet text which enables it to generate responses that sound human-like. When a sequence is given, it calculates the probability of each possible next word and selects the one with the highest likelihood. This continues until a full response is generated. The model uses attention mechanisms to determine how much importance should be given to each word in the input when predicting the next word. It's like an incredibly vast and complex probability machine, estimating which word is most likely to come next given all the previous ones.