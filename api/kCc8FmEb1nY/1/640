'Sentence piece' refers to a commonly used strategy to encode and decode text into integers for training transformer-based language models. Unlike the character-level tokenizer described in the context, which translates individual characters into integers, SentencePiece is a sub-word tokenizer. This means that it translates chunks of words or 'subwords' into integers, not just individual characters. The 'subword' unit is chosen because it strikes a balance between word level (which can be too large) and character level (which can be too small), hence capturing more linguistic information while maintaining computational efficiency. SentencePiece is particularly useful when dealing with languages that do not use spaces to distinguish words, or when the model needs to understand subword meanings in larger corpora of text.