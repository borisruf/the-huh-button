This statement is explaining that the training code for Transformer AI models is available in the GitHub repository named Nano GPT. The code is reasonably simple and contained in just two files, each containing approximately 300 lines of code. These files contain the code necessary for training the Transformer on any given text, which can be particularly helpful for understanding the underpinnings of more complex language models such as ChatGPT.