In OpenAI's language model ChatGPT, it generates language based on tokens. These tokens can be thought of as small sub-word pieces. The model analyzes previous tokens in a given sequence, and then predicts the next token in that sequence. This process repeats until the model outputs a full sentence or paragraph. While a token could represent a full word, it often represent sub-words or characters, particularly for languages with complex combinations or non-English languages. For instance, a word like 'chatting' could be split into multiple tokens, such as 'chat' and 'ting'. These smaller parts make the model more efficient and adaptable to different languages and styles of text.