Andrej proposes to explain how Chat GPT works by training a Transformer-based language model on a small dataset called 'Tiny Shakespeare,' which is a file containing all of Shakespeare's works. By studying the patterns in this data, Andrej hopes to model how the characters follow each other and eventually generate an infinite stream of text mimicking Shakespeare's language. He plans to conduct this exercise from scratch using Python, taking viewers through the process of defining and training a Transformer. He asserts that this approach will help viewers understand how Chat GPT works at the structural level. His goal is not to reproduce Chat GPT, but to create a simple, educational model. To assist in this process, he has developed a code repository called 'Nano GPT' through which he can train Transformers on different texts.