The presenter is explaining that in the lecture, they are going to focus on Transformer-based character-level language model using the 'Tiny Shakespeare' dataset. Given a specific character sequence from Shakespeare's works, the model will predict the next character in the sequence. Then, using this model, an infinite amount of Shakespeare-like text can be produced. To convert the raw text into sequences of integers, a process called tokenization is used. In this case, a character-level tokenizer is used, which maps each character to a unique integer. But the presenter also discusses other types of tokenizers for more complex models like Google's 'sentence piece' and OpenAI's 'Tick Token'. However, the lecture is going to stick with the character-level tokenizer for simplicity.