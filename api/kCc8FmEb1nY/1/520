In Python, text is represented as a sequence of characters. The presenter proposes to work with the Tiny Shakespeare dataset to train a 'character-level language model' with a Transformer neural network. This involves learning how one character relates or follows another in sequence from the entire works of Shakespeare. The presenter aims to slowly build the Transformer from scratch in a Jupyter Notebook, training it on the dataset to generate infinite sequences that imitate the style and language of Shakespeare.