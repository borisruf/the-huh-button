Andrej proposes to create a simplified version of a Transformer-based language model using a dataset known as "Tiny Shakespeare". While he acknowledges that this won't replicate the complexity of Chat GPT precisely, he believes it will highlight the principles of how such systems work. He later demonstrates this concept using a preliminary version of Nano GPT stored in a GitHub repository. He has previously trained this Transformer model on an OpenWebText dataset and managed to reproduce the performance of GPT-2. To educate his audience about the inner workings of Chat GPT, Andrej plans to construct this repository from scratch during the video, using a Google Colab Jupyter notebook.