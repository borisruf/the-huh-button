Andrej shares that he will focus on training a Transformer-based language model by using the 'Tiny Shakespeare' data set. He explains that by using a sequence of characters from Shakespeare's works, the Transformer neural network will model the pattern and predict the next possible character in the sequence. Once the system is trained, it can generate an endless string of text that resembles Shakespeareâ€™s writing style. Although the system is not perfect and may produce words that don't make sense, it simulates how the AI language model, ChatGPT, works. Andrej shows that his code for this process is available in a repository called 'Nano Gpt' on GitHub. To conclude, he clarifies that his primary objective is to make his audience understand how a language model like Chat GPT works using simple constructs that can be applied to any text dataset.