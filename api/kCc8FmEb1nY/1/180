The statement is referring to the significant impact of the Transformer architecture, initially proposed for machine translation tasks in the "Attention is all you need" paper in 2017. This new architecture became extremely influential and widely used in the AI field beyond machine translation. It changed how neural networks model sequences of words or tokens and fundamentally influenced how AI understands and generates human language. In the five years that followed its introduction, Transformer-based models like ChatGPT became prevalent and used for a wide range of AI tasks and applications, from text generation to image recognition and beyond.