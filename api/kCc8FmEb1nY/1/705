Andrej aims to demonstrate and explain the working principles of Chat GPT by creating a smaller, simpler version using the Transformer architecture. However, he acknowledges that he will not be able to reproduce Chat GPT exactly due to its complexity, extensive training process, and large textual data derived from the internet. Instead, he will train a Transformer-based language model on 'Tiny Shakespeare', a compact dataset of all the works of Shakespeare. The model will predict the next character in a sequence by identifying patterns in the dataset. Though this model will produce "fake" Shakespeare, Andrej assures that it will resemble the language and style of Shakespeare. He has previously written code to train Transformers, available in a GitHub repository called 'nano GPT'. After demonstrating the process with 'Tiny Shakespeare', viewers can apply the same methodology to any text dataset of their choice. His ultimate goal is to provide a greater understanding of how Chat GPT operates.