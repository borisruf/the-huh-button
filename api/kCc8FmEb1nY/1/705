The text is explaining the differences between character-level and sub-word tokenization in natural language processing. Character-level tokenization splits the text into separate characters, while sub-word encoding divides the text into sub-word units (i.e., not encoding whole words, but also not encoding individual characters). The text chooses to use character level for simplicity. It also provides examples of tools like OpenAI's "Tick Token" library which uses a byte pair encoding tokenizer usually adopted in practice and Google's "sentence piece" that encodes sub word units.