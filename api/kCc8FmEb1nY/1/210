In the provided context, the speaker talks about developing a basic version of the technology that powers ChatGPT, which is a robust, complex language model. They want to train a Transformer-based language model. However, they clarify that their model will work on a simpler, character-level. This means that instead of predicting the probability of the next word given the words so far (which is how most language models, including GPT, work), their model will predict the next character based on the characters seen so far. The context suggests that the character-level model is a way to simplify the task and more easily explain how language models work, without needing to replicate the entire complexity of a system like ChatGPT.