In this lecture about character-level language model built using a Transformer, the speaker articulates that while they won't reproduce the complex GPT model (generatively pre-trained Transformer), they will focus on training a simpler, character-level language model with the Transformer architecture using the 'Tiny Shakespeare' data set. 

By doing so, they hope to give the audience an understanding of how the underlying mechanics function in creating a language model. The dataset comprises all of Shakespeare's works, and the model will be trained to predict the next character based on the previous ones, thereby modeling all patterns in the data. Once trained, the model will be able to generate infinite text that emulates Shakespeare's style.

This explanation leads the speaker to introduce the Python programming language, a popular language used in AI and machine learning. They explain that they are going to handle the 'Tiny Shakespeare' data set as a sequence of characters in Python.