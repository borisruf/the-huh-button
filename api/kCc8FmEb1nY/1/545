In the broad context of machine learning and AI, the term "vocabulary" generally refers to the set of unique words / terms that the model has been trained on and is capable of understanding. With regards to languages, it would usually mean unique words, but in this context of the Tiny Shakespeare dataset, it refers to the unique characters that appear in it. 

The vocabulary size, hence, represents the number of unique characters in the dataset. This would include everything from alphabets, numeric digits, punctuation, and even whitespace - any unique character present in the text data. 

A larger vocabulary size would lead to a more complex model that can understand a larger range of inputs, but might also require more computational resources and training time. Therefore, understanding and managing the vocabulary size is important while constructing and training language models, such as the Transformer model for the ChatGPT they are discussing here. 

They would convert each unique character in the dataset into a distinct integer (or token), thereby creating numeric representations of the input data that the model can work with. The total number of unique characters would represent the vocabulary size of the model.