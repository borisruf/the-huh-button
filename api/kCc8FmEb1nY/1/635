SentencePiece is a language-independent subword tokenizer and detokenizer designed for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. The concept of subword tokenization involves breaking words into smaller units (subwords), which allows the handling of words not seen during training, better handling of rare words, and reduced vocabulary size. SentencePiece implements subword regularization which is a variant of Byte Pair Encoding (BPE) and unigram language model with the extension of direct training from raw sentences. This means it tokenizes a sentence into a sequence of subword units, not necessarily following the natural delineation of words in a sentence but based on the frequency of occurrence of the subwords in the training text. It's a kind of merger between Bag of Words (BOW) and the fixed vocabulary methods.