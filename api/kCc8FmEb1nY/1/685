The lecturer is explaining how different text tokenizers work. In his example, he uses a character-based tokenizer that encodes each individual character into an integer, resulting in a list of integers representing the original text. The integers are within the range of 0 to 64, according to the specific set of characters used in a given data set.

He then contrasts this with Google's SentencePiece and OpenAI's Tick Token tokenizers, which operate on a sub-word level instead of a character level. In this case, the encoding results in fewer integers (such as only three for the phrase 'hi there') because it considers parts of words, or "tokens", rather than individual characters. This results in a range of possible integers between 0 and 50,256, as the greater granularity of sub-word encoding requires a much larger vocabulary. The specific range depends on the total number of unique sub-word units in the tokenizer's vocabulary.