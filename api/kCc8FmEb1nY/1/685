In the context of machine learning models for text, 'tokenizing' refers to converting raw text into a sequence of integers, which are coded representations of the words, sentences, or in this case, characters. Each integer represents a unique word or character, and models use these integers to identify and generate text.

In the context above, a character-level model is being created for the AI. This means that the model breaks down and understands the text data at the level of individual characters (letters, numbers, symbols, etc.). Another common level for tokenizing or understanding text data is at the 'sub-word' level, which is also referred to as a 'sentence piece'. This method involves breaking text data down into chunks that are smaller than a word but larger than an individual character.

In a nutshell, 'sub-word unit level' in machine learning usually refers to a middle ground between character-level and word-level data processing. This level can often provide a balance between computational complexity and the ability to understand nuances in the text, and is commonly used in practice.