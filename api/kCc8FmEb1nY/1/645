In his video, Andrej shares his plan to explain and create a basic version of a Transformer-based language model, similar to but much simpler than ChatGPT. To do this, he opts to use a smaller dataset ('Tiny Shakespeare') that consists of the works of Shakespeare. He outlines that the goal of the model is to predict what character comes next based on a given sequence, thus the character sequences would mimic Shakespeare's writing. Andrej points out that the output from his model will be on a character-by-character basis, whereas ChatGPT produces output on a token (word chunk) level. Furthermore, he mentions that he has pre-written and shared the essential code for training Transformers online, through a repository called 'nano GPT'. Andr√©'s motivation for doing all this is to help his viewers understand what goes on behind the scenes in larger, more complex language models like ChatGPT. He then sets out to construct the Transformer model from scratch, using a Google Colab notebook.