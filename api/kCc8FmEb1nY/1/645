The passage is talking about different methods of tokenizing text when working with language models like ChatGPT. Tokenizing involves converting raw text into a sequence of integers, each integer representing a specific word, character, or part of a word. This is necessary to enable the model to process and understand the text. 

In this context, the author mentions how they're using a simple character-level tokenizer which assigns an integer to each individual character. Every unique character is treated as a different 'token' and assigned a specific integer value. The model would learn the sequence of these characters, allowing it to generate outputs which will follow the same patterns as it has learned from the input data.

However, there are other forms of tokenization as well. One such method is 'sentence piece' tokenizer used by Google, which works at a 'sub-word' level, meaning it breaks down the text into smaller parts that are larger than individual characters but smaller than complete words. This method is often used in practice due to its efficiency and ability to handle a wider range of linguistic variations.