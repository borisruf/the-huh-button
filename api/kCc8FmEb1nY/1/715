The statement is about tokenization, the process of breaking down texts into smaller units, such as characters or words, when training complex language models like GPT-2. These smaller units of text are referred to as tokens. The vocabulary of possible tokens is wider than just the characters in the English alphabet. For GPT-2, instead of having just 65 possible tokens (one for each character), there are 50,000 tokens in its vocabulary. This is due to the use of subword tokenization, which can better handle a wider range of words, grammatical endings, word roots, etc. in a more efficient manner for understanding and generating language. This type of tokenization can lead to more accurate and sophisticated language generation, as it affords the model a richer understanding of the semantic structures of language when compared to simple character tokenization.