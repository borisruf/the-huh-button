The speaker, author of 'Nano GPT', discusses how ChatGPT works on a basic level and presents the intention to demonstrate how to build a character-based language model using their own simplified but functional code. They underscore that the purpose is to help the audience understand the underlying mechanisms and techniques of ChatGPT, which is based on the Transformer architecture proposed by the 2017 research paper 'Attention is all you need'. In the presentation, the speaker plans to train their language model on the 'Tiny Shakespeare' dataset which contains packed works of Shakespeare, and uses it to predict the sequential characters in a text as an example of how ChatGPT generates responses to different input sequences. The speaker affirms that their simplified model will generate an "infinite" Shakespeare-like character sequence in a manner quite similar to how ChatGPT works.