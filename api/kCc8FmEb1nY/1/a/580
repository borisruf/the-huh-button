To tokenize the input text, firstly we need to translate characters into tokens we can feed into the model. We also need to conversely translate tokenized outputs into human-readable text. When we translate the characters to tokens, this would usually involve creating dictionaries of keys as the characters and values as unique integers representing each character. Similarly, we translate tokens to characters by creating dictionaries with keys as unique integers and values as their corresponding characters. The tokenization strategy is essential as the model understands only numeric representations, not alphanumeric characters. It is required for preparing the data before training the model as well as for interpreting the model's output post-training.