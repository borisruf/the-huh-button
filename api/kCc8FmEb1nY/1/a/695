The speaker is using OpenAI's library "tick token". This library uses a "pipe pair encoding tokenizer", which is what the GPT model uses. A tokenizer is a means of segmenting text into words, phrases, symbols or other meaningful elements called tokens. These tokens help in understanding the text's context, which is an essential aspect of Natural Language Processing (NLP). In the "pipe pair encoding tokenizer" method, words are initially segmented based on spaces and punctuation. The most frequent sequences of characters, like words or parts of words, are then gradually learned and used as tokens, resulting in tokens that represent whole words and subwords.