In the context provided, 'sentence piece' refers to a tokenization system developed by Google. Traditional tokenization methods split texts into units such as words or individual characters, but the 'sentence piece' system operates at the subword level. This method was primarily intended for machine learning tasks, particularly Machine Translation and Natural Language Processing (NLP). 'Sentence piece' allows a model to handle rare words and out-of-vocabulary words effectively, as it breaks down words into smaller, manageable tokens. This method also enables the handling of multiple languages without needing to design language-specific rules.