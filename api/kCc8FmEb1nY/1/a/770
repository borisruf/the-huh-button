The person is detailing the basics and workings of chatbots, mainly focusing on ChatGPT. They explain how it can generate different responses with the same prompt, because it is a probabilistic system. They give numerous examples of the tasks ChatGPT can perform, like composing poems or writing news articles, and describe it as a language model which completes given sequences of words. 

The Tranformer neural network underpins ChatGPT, initially developed for machine translation but became widely used in AI. The individual plans to explain how to build a smaller version of ChatGPT using the Transformer. Their program will use a character-level language model instead of a word-level one, and the training will be done on a small dataset called Tiny Shakespeare, aiming to predict the next character in a sequence. 

They highlight that understanding the program does not require deep knowledge of AI, but basic understanding of Python, calculus and statistics will be helpful. Also mentioned is the concept of tokenizing, which is transforming text into integers, explaining there are different methods of doing this. They decide to use a simpler method, character-level tokenizer, despite it resulting in larger sequences. Their main aim is to keep things simple and understandable.