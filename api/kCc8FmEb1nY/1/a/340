The speaker in the above context is explaining the working of a Transformer-based language model architecture. He is contrasting his character level language model, which processes one character at a time, with ChatGPT operating at a 'token' level. In this context, 'token' refers to chunks of a text that can be smaller or larger than a word. So, instead of generating predictions based on individual characters or complete words, this kind of language model operates at the 'token level', which is essentially a 'word chunk level'. The model predicts the next 'token', which could be a part of a word, a whole word or even multiple words, based on the existing sequence of tokens. This allows the model to generate more complex and grammatically correct text.