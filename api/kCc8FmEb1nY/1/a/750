The presenter is explaining the difference between the types of tokenization methods used in neural nets like OpenAI's ChatGPT model. Tokenization is the process of converting text, a sequence of characters, into a sequence of integers or 'tokens'.

In this video, the presenter demonstrates a simple character-level tokenization method where each character (including state characters, special characters, and capital and lowercase letters) is represented by a unique integer. This creates a list of integers that can be encoded from and decoded back into its original string form.

However, the presenter points out that this is only one type of tokenization used. Other tokenization methods might involve encoding larger units of text â€” like whole words or 'sub-word' units using methods like 'sentence piece' or 'byte pair encoding'. These alternative encoding methods involve trading off the codebook size and the sequence length, allowing for shorter sequences of integers but with larger vocabularies, which makes them more common in practice.