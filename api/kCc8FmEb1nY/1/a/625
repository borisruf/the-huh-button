The speaker is explaining how tokenizing works in a character level language model. Tokenizing is the conversion of raw text into a sequence of integers based on a vocabulary of possible elements. Since it's a character level language model, individual characters are translated into integers. An example is given, where the phrase "hi there" is encoded into a series of integers, each representing a character in the phrase. This list can be decoded back into the original string, showing the bi-directional capability of this tokenizing/encoding system, an essential part of neural network training.