A 'sentence piece' tokenizer is a type of program that breaks down text into smaller units than a single word or character, usually a chunk of a word. It acts on text by encoding it into integers and helps process languages that do not use space for word separation. The phases of this tokenizer (tokenization and detokenization) facilitate processing complex linguistic data and building a more accurate language model. This tokenizer is useful for the automatic processing of natural language, as it can balance between character and word-level granularity.