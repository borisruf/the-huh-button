The speaker is about to embark on explaining how to build a 'Transformer'-based language model from scratch, specifically one like ChatGPT, which completes sequences of characters or words based on patterns it's learned from its training data. Noting that constructing a complete AI model, like ChatGPT, would be an impossible feat for their case due to its complexity, they decide to train a simpler 'character-level' language model on a small dataset of Shakespeare's work named 'Tiny Shakespeare'. This model is considered to be beneficial for learning how these systems operate. The idea is that after observing previous characters in a sequence, the Transformer model will predict the next likely character.