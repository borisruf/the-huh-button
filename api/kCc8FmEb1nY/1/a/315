The speaker is explaining how they plan to train a character-level language model using a Transformer, an architecture born from a 2017 paper titled 'Attention is all you need.' They plan to train this model on the Tiny Shakespeare dataset, which is essentially all of Shakespeare's works in one file. The idea is to train the model to predict what character is most likely to come next given a sequence of previous characters. So if given a context of certain characters, the model predicts that 'g' is most likely the next character. Once trained, the model should be able to generate Shakespeare-like language. But note, this would be an approximation and not perfect Shakespeare.