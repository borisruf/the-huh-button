In this explanatory lecture, the speaker outlines how to create a Character-Level Language Model using a transformer architecture. The aim is to provide a comprehensible understanding of how GPT (Generative Pretraining Transformer) such as GPT-3 from OpenAI works. The language model will be trained using the 'tiny Shakespeare' dataset. It will be a simple implementation without the intricacies of larger models like GPT-3. Yet, it will illustrate how these more complex models function at a basic level.

The first step is encoding and decoding text as integers using a character level tokenizer (although in practice, sub-word encodings are used more often). This step translates the raw text into a sequence of integers. The speaker aims to build both an encoder, which will convert a string into a list of integers, and a decoder, which will convert the list back into the original string. 

The key takeaway from this lecture is understanding and appreciating the mechanics underpinning chatbot models such as ChatGPT. Using this understanding, you should be able to write your own simple transformer-based language model.