The speaker is explaining the process of tokenization in the context of training a language model, specifically, transforming raw text into a sequence of integers. He notes that this is a simplified version of tokenization, where each character (including spaces and punctuation marks) in the text is mapped to a unique integer. The mapping to integers is arbitrary. This sequence of integers is then used to train their language model. This approach simplifies the tokenization process but results in longer sequences because it operates at the character-level. It is noted that in practice, more complex tokenization strategies such as sub-word unit level and word level are commonly used. He concludes by demonstrating how the textual data has been transformed into a sequence of integers for the language model. The speaker also references PyTorch, a python-based machine learning library, which is used in the tokenization process.