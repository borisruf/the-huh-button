In this explanation, the author is discussing the use of encoders and decoders, also known as a tokenizer, to convert a large text dataset, in this case, the works of Shakespeare, into a form that a machine learning model can understand and work with. The process of tokenization involves converting the raw text data to a sequence of integers according to a vocabulary of possible elements. This allows the model to better analyze and generate text based on patterns it identifies during training. The author uses the example of a character-level tokenizer where each character is translated into an integer. However, they also mention more complex schemas like sub-word level tokenization which is often used in practice.