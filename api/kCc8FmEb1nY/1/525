In this text, the author is explaining an experiment with Chat GPT, an AI language model, with an emphasis on its probabilistic system. The author shows off numerous humorous text-based prompts and explains that each prompt may have slightly different outcome, depending the AI's interpretation. Following some examples of prompts and generated responses, the author explains that Chat GPT works by modeling sequences of words, characters, and tokens. 

The author specifies that he's going to simplify the process for the purposes of an exercise by teaching a smaller version of the Transformer model to follow a specific sequence of characters from a dataset called Tiny Shakespeare, a miniature version of all Shakespeare's works. 

The author further elaborates on how the project will work. Given a sequence of characters, the model should be able to complete the sequence based on the patterns it has learned from studying Tiny Shakespeare. The successful output will be a character-by-character imitation of Shakespeare's writing. 

The author then points out that the code for a working version of this model already exists in a GitHub repository called 'nanoGPT'. In the current lecture, the author intends to re-create this repository from scratch in order to teach the audience how it works.

Finally, the author concludes that he's already loaded Tiny Shakespeare's data into a Python script and by invoking a specific function has determined that the dataset contains 65 distinct characters.