This statement is referring to the content of the original academic paper that introduced the Transformer model, titled 'Attention is all you need.' The speaker indicates that for the uninitiated reader, the paper may seem confusing or lacking in coherence, like a badly machine-translated document, due to its highly technical language and advanced concepts. This is an analogy. It's not saying the paper is poorly written, rather it is highlighting the complexity and the high level of expertise in the field required to understand it.