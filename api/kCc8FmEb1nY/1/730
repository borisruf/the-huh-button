The lengthy explanation discusses how to use machine learning to build language models, specifically focusing on a model known as ChatGPT. The distinctive advantage of ChatGPT is it's a probabilistic system which can produce multiple outputs given the same prompt, allowing for more varied and dynamic interactions.

The author goes on to explain that the backend of ChatGPT is based on a Transformer architecture, a concept developed in the monumental paper, 'Attention is All You Need'. Transformer models have become a favoured choice in machine learning applications due to their efficacy and versatility.

The author presents an example of building a language model using a smaller dataset, 'Tiny Shakespeare', which is a compilation of all the works of Shakespeare. The aim is to train a Transformer model to understand the sequential nature of characters in the Shakespeare text so that it can predict the next likely character.

The author maintains a simple implementation for the benefit of beginners. The process includes tokenising the text to convert raw string data into a series of integers, with each character given a unique numerical value. This creates a two-sided mapping that allows text to be converted into a numerical sequence and then decoded back into the original text. The author concludes by stating that while this basic method deals with very long sequences and small code books, more complex techniques can be employed for larger code books and shorter sequences.