The speaker is explaining how GPT-powered chat models work. For this, they present an example: using a character-based language model to predict the next letter in a sequence from the works of Shakespeare. They note that while they use character-level tokenization to translate each character into a unique integer, there are other methodologies, like Google's SentencePiece or OpenAI's Byte Pair encoding, which tokenize at the intermediate "sub-word" level. These methods, while more complex, can trade sequence length for vocabulary size, giving shorter sequences of integers but with a much larger range of possible values.