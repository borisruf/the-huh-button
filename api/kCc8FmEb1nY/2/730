This passage seems to be from a lecture on how the ChatGPT AI model works. It explains how this AI model "learns" and generates language using what's called a Transformer architecture. The speaker then goes on to narrate how they want to demonstrate this by creating a simpler demonstration model that generates text in the style of Shakespeare using the Transformer approach. 

This involves picking a "vocabulary" of characters from a digitized Shakespearean text, assigning each character a unique number ("tokenizing"), and then training the AI model to predict which character comes next based on what it "reads" in the text. The result is a model that can generate its own "Shakespearean" sentences. 

Then the lecturer compares this simple model with what a more complex AI model does in reality, the latter having a much larger and complex 'vocabulary' to generate more varied sentences and phrases. The goal is to help viewers better understand the underlying mechanisms of AI text generation models like ChatGPT.