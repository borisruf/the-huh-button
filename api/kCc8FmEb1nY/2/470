In this part, the speaker is discussing about training a 'Transformer based language model'. He has downloaded his favorite toy data set 'Tiny Shakespeare' which contains all works of Shakespeare. The model will then try to predict what comes next in a sequence of characters. It will be trained on this dataset and will try to produce character sequences that resemble Shakespeare's writing. The speaker further mentions he has already written the code to train these Transformers, and it's available in a GitHub repository known as 'Nano GPT'. This repository contains the program for training Transformers on any given text, and he will explain how to do it in the tutorial. He has also created a new Google Colab Jupyter notebook to easily share the code for users to follow along. Finally, he notes that he has downloaded the Tiny Shakespeare data set and will now open the input to begin the training process.