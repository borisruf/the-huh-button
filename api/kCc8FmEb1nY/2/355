The speaker is discussing the workings of ChatGPT, an AI-based language model that can generate different responses to a given input. They highlight that while the technology is complex, at its core it uses a neural network called 'Transformer' based on a 2017 paper titled "Attention is All You Need". The speaker then proposes to demonstrate how to train a simple Transformer-based character language model using the 'Tiny Shakespeare' dataset. The goal is to predict the next character in a sequence, learning patterns from the dataset to generate Shakespeare-like texts. They've created a basic implementation of training Transformers in a GitHub repository called 'Nano GPT'.