Tokenizing is the process of converting raw text into a sequence of integers, according to a created index or vocabulary of possible elements. In the given context, it is about transforming individual characters in a text into integer form. 

For example, to develop a language model that predicts characters in a text document (like Shakespeare's play), this transformation process is necessary. The language model 'learns' and 'predicts' based on the numerical representations (tokenized forms) of the characters, not the actual text characters themselves. So, each character in the text will be assigned a unique integer which the model uses for the prediction process.