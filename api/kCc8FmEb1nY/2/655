The text talks about how GPT (Generatively Pre-Trained Transformer) works, created by OpenAI. GPT is a system that allows us to interact with AI and assign it text-based tasks like writing a poem and providing answers to prompts. GPT generates the sequence of words or characters to answer these prompts. 

The discussion then goes deeper into the architecture of GPT; it is based on a system called Transformer, which was initially invented for machine translation. However, it has been adopted in other AI applications. 

The speaker also discussed teaching the AI to write like Shakespeare with a character-level language model using the Transformer and how to translate the raw text into a sequence of integers, a process known as tokenization. Tokenization breaks the content into individual words or even parts of words (known as sub-words) to create a vocabulary of possible elements.