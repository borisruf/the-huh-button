This excerpt is about the ChatGPT AI, which has become quite popular for its ability to generate text. It models the sequence of words, characters, or tokens (sections of words), in the English language, using a given prompt to generate a set of probable responses. The architecture behind ChatGPT is called Transformer and it came from a paper titled "Attention is All You Need" in 2017. This AI has been trained on a large amount of internet text, but in this demonstration, a smaller dataset called 'Tiny Shakespeare' is used. The text from Shakespeare's work is converted into characters, which are then modeled by the Transformer neural network to predict the likely next character in the sequence, thereby generating Shakespeare-like language. In this model, the characters are broken down into smaller token units, rather than entire words, a process known as 'sub-word' tokenizing. This level of encoding is often used in practice.