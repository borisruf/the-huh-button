The passage is explaining how to process text data for use in a Transformer - a type of AI model used for tasks like translation or text generation. 

Firstly, it involves gathering all the unique characters used in the entire dataset - every single letter, punctuation mark, or symbol. It also arranges them in a sorted order. 

The total number of these unique characters is then measured, and this forms the "vocabulary size" of the dataset. This refers to the total variety of characters which our model will have to understand and utilize. 

So in short, we're preparing all the different 'building blocks' the Transformer will have to work with in generating or understanding text.