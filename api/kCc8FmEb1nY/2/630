ChatGPT is a language model created by OpenAI that generates text in response to a given prompt. It is probabilistic, meaning for any given prompt, it can generate many different but accurate responses. A common method for training language model systems includes "Transformer", an architecture proposed in a paper titled "Attention is All You Need". This architecture is the main component of the ChatGPT system. 

The lecture discusses the process of creating a simplified version of a transformer-based language model using a small dataset, known as 'Tiny Shakespeare'. The aim is to model how Shakespeare's language sequence followed each other. Using this model, you can then generate text that appears Shakespearian. 

The system uses tokens, which are sub-words, to produce the sequence of words. One way to train such models is to convert raw text into integers according to a vocabulary. For instance, in the lecture, individual character to integer conversions are used (like 'a' to 1, 'b' to 2, etc.) This process is known as tokenization. There are different schemas used in real-world applications, such as Google's "Sentence Piece", where the tokenization is done at the sub-word level.