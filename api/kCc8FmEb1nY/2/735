The speaker is explaining how the GPT models work on string/ text data. The string of characters or words is converted to a series of integers for processing. The specific transformation is known as encoding, and the process of transferring it back to text is called decoding.

He provides an example, converting the string "Hi there" into integers. In his process, each letter is turned into an integer, giving a list of several numbers.

However, he mentions other methods. For instance, GPT-2 uses a 'pipe pair encoding tokenizer' which maps chunks of words (known as 'tokens') into integers. This is more efficient, as the string "Hi there" is then mapped to just three integers. Although, because this method uses more possible tokens (up to 50,000 compared to the 65 individual characters of the English language), the corresponding integers are also larger.