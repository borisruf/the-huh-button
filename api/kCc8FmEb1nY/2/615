This text explains how text can be tokenized for use in AI programs like ChatGPT. Tokenization is the process of converting raw text into a sequence of integers according to a vocabulary. For example, in a character level language model, individual letters are translated into numbers. This process is essential for AI systems to understand and work with text data. To demonstrate, the explanation includes a sample of text, "hi there", and shows that its tokenized form will look like a series of numbers, such as "46, 47, etc."