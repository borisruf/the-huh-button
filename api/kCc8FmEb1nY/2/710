In machine learning and AI, a tokenizer breaks up text into smaller pieces called tokens. These tokens can be on a character level (each character is a token), word level (each word is a token), or sub-word level (parts of words are tokens). This process is known as tokenization and it helps computers understand language by converting it into numerical format. 

Now, let's talk about context. We're utilizing a tool named ChatGPT to generate text based on the patterns it has studied in the English language. It's a probabilistic system, meaning it observes past sequence of words and predicts the next word based on probability. Like predicting what letter follows after a certain sequence of alphabets.

In our case, we're using tokenization at a character level - each character in a text is a unique token. We chose to keep it simple due to the complexity and vastness of tokenizing on a word level or sub-word level. I'll show you how to train this text generator using the compilations of Shakespeare - an exercise that'll hopefully help you grasp the intricacies of how AI systems like ChatGPT works.