In simple terms, the speaker is explaining that in order to train artificial intelligence (AI) systems like ChatGPT, textual data is converted into numerical data. This is done using a process called tokenization in which each unique word or character in the text is assigned a unique integer. This makes it easier for algorithms to process and learn from the text. The speaker demonstrates this process using several pieces of code and points out different methods for tokenization. For example, one method is to assign integers to each unique character, another is to assign integers to each unique word, and a third method is to assign integers to sub-word units. These different approaches can provide different levels of granularity and complexity to the AI's understanding of the text.