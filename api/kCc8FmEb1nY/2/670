The instructor is providing a simplified explanation of how the OpenAI's GPT-2 model works. GPT-2 uses a method called 'encoding' where it converts text into integers for processing. In case of GPT-2, instead of 65 tokens or units of information (which was used in a basic character-level language model used as an example), there are 50,000 tokens that include words, phrases, and letters. This helps GPT-2 to generate more complex outputs, like the natural language responses and tasks it is renowned for.