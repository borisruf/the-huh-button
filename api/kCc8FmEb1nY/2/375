The speaker is talking about ChatGPT, an AI that can generate responses to text-based prompts. It can produce different responses to the same prompt because it's a probability-based system. The AI uses a process called the Transformer architecture, proposed by a 2017 paper, to generate its responses. 

The speaker wants to create a simpler version that uses the Transformer method. They propose using a small dataset called Tiny Shakespeare, which contains all of Shakespeare's works in a single document. Their system will predict the next character in a sequence based on its training on this dataset.

They've written the code for training Transformers in a GitHub repository called Nano GPT. It has a simple implementation, consisting of just two files of 300 lines of code each.