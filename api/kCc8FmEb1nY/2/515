The speaker describes the capabilities of a well-known AI system known as ChatGPT, which can generate text based on given prompts. The speaker notes that the technical basis for ChatGPT is a transformer model, which was introduced in a 2017 paper. This model predicts what comes next in a sequence of words, helping to produce human-like text. 

To further demonstrate how a transformer model works, the speaker intends to create a smaller, simpler version, using a dataset of Shakespeare's works. This lesser model (called 'Tiny Shakespeare') is a character-level language model that predicts which character is likely to come next after a given string. 

This allows it to produce text that mimics the style of Shakespeare, through studying and learning the patterns of characters in his literature. The speaker mentions that this activity is for educational purposes and will contribute to a better understanding of how the larger AI systems, like ChatGPT, operate. Lastly, a reference was made to a code these methods have been implemented in, available on the speaker's GitHub, called Nano GPT. The speaker plans to provide a step-by-step demonstration of how the model is created from scratch using Google's Colab Notebooks.