In this context, the speaker is talking about different ways that a text can be transformed into numbers by a tokenizer. They share that they use a simple technique that translates individual characters into integers. This is done by creating a lookup table from the character to the integer and vice versa. When you encode a string, it translate all the characters individually. There's also a reverse mapping to convert it back into the original string. This way of encoding is just one of many possible methods. Other methods might encode whole words or parts of words into integers. An example is OpenAI's 'Tick Token' that uses a byte pair encoding tokenizer.

When talking about trading off the code book size and sequence lengths, the reference is to the choice between having many tokens (such as characters in the simple method or words in other methods) which results in longer sequences of numbers or having a smaller number of tokens (such as parts of words in other methods) resulting in shorter sequences of numbers.