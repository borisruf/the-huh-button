The speaker, Andrej, talks about creating a language model using a dataset called 'Tiny Shakespeare'. He will train a neural network called a 'Transformer' to generate text based on patterns it learns from this dataset. This process involves giving the Transformer system a sequence of characters, and training it to predict what character should come next based on the pattern it identifies in the previous sequence. Andrej mentions that this model will be simpler than the complex ChatGPT system, but aims to offer a basic understanding of how language models and transformers work. The end goal, he shares, is to generate 'infinite Shakespeare', or text that looks like it could have been written by Shakespeare. His coding work for this project is available on Github under the repository named 'nano GPT'.