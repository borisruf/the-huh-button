In simple terms, the speaker is explaining the process of training an AI (Artificial Intelligence) model to learn and generate text like a human would. This is done by programming the AI to learn from a large collection of text (like a book or the internet) and understand the pattern or sequence of words and characters in this text. Then, using this understanding or "knowledge," the AI is able to generate its own text that follows a similar pattern or sequence.

The speaker uses the concept of 'tokenization', which basically means transforming the raw text into a sequence of numbers. This is done so that a machine (a computer) can understand and process the information. The speaker illustrates this using individual characters (like letters) as the smallest unit or 'token'. 

So, for the model, each unique character in the text is represented by a unique number. When you give the model a string of characters ("hi there" for instance), it translates or encodes it into a list of corresponding numbers. And it also does the reverse, decoding a list of numbers back into a corresponding string of text. This way, the model learns to predict what character (or number) comes next in a sequence, helping it generate its own text.