ChatGPT is an AI language model by OpenAI that can generate paragraphs of text similar to how a human would write, given an input or "prompt". It learns this skill by being trained on large amounts of text from the internet. The text generation process happens sequentially, from left to right, one word at a time.

The neural network, or the 'brain', behind ChatGPT is based on the Transformer architecture, a significant development in AI research. Transformers have been used in many different AI applications, and in the case of ChatGPT, it primarily helps in predicting what word comes next in a sentence.

To understand how Transformers work, an easier example than the complex ChatGPT is demonstrated. A smaller version of a Transformer-based language model is created that predicts not words, but characters in a text - a character-level language model. This simplified model uses the works of Shakespeare as its training data. 

So, like ChatGPT, this smaller model takes in an initial sequence of characters, predicts what character comes next, and repeat this process to generate a sequence of characters. This new sequence should resemble the style of Shakespeare's writings. The text generated by this model is not perfect but can generate Shakespeare-like language, showing us how Transformers and language models work.