The speaker is explaining how to build a language model using a framework called a Transformer, using Shakespeare's complete works as a training dataset. This model will be trained to predict the next character in a sequence of text. He mentions that the complete code for this demonstration can be found on his GitHub repository, but in this lecture, he will start from scratch, training the model using a Jupyter Notebook. He also downloaded the 'Tiny Shakespeare' dataset and printed the first 1000 characters as part of the preliminary setup. Then, he shows how to convert the raw text into a set and list of unique, sorted characters, which serves as the vocabulary the model will work with. According to the speaker, this sequence of unique characters (the vocabulary) will eventually be converted into integers through a process called tokenization, which is the next step in the explanation.