The speaker is discussing how they have created a simpler training model for AI language models based on the Transformers introduced in a 2017 paper titled 'Attention is all you need'. 

They trained this simpler model, which they called Nano GPT, on a dataset called Tiny Shakespeare, which contains all of Shakespeare's works. This process involved having the model predict the next character in sequences taken from the dataset. The end result is a model that can generate text that resembles Shakespeare's writing style since it learned from his works.

The speaker then compares Nano GPT to Chat GPT, a more complex language model developed by OpenAI. Like Nano GPT, Chat GPT generates text by predicting the following sequence of words, or tokens, based on previously given input. However, instead of being trained on Shakespeare's works, Chat GPT was trained on a larger dataset from the Internet.

The speaker emphasizes the simplicity of Nano GPT, mentioning that it only requires two files of 300 lines of code each: one to define the model and the other to train it. This is significantly less complicated than producing a system like Chat GPT. Nevertheless, the speaker demonstrates that the Nano GPT can reproduce the performance of GPT2 when trained on the Open Webtext dataset.