The neural network behind ChatGPT that helps model the sequence of words is called a transformer model. These models are powerful because they can process data in parallel and also keep track of long-range dependencies in a sequence. They essentially work by analyzing a group of words or symbols together and predicting what comes next. This helps the program to understand language patterns and generate coherent and contextually accurate responses. The transformer model uses layers of artificial neurons or 'nodes', trained on massive amounts of text, to predict these sequences or 'completions'. This is what allows ChatGPT to respond to prompts in a way that simulates human conversation.