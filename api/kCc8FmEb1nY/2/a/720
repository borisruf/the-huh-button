The speaker wants to use the same encoding method that was used to train GPT-2. Instead of just having 65 possible characters or tokens, there are 50,000 tokens. When encoding the text "High there", as an example, they end up with a list of three integers. However, unlike their character-level encoding method (which provides numbers between 0 and 64), these integers can take any value between 0 and 49,999.