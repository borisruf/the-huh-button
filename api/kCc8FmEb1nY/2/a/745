In this excerpt, the author talks about training a Transformer-based language model using the Tiny Shakespeare dataset. He employs a character-level language model rather than a word or sub-word model. Then he introduces the concept of tokenization, which is a process where we convert the input text to a sequence of integers according to a vocabulary of possible elements. His example uses character level tokenization where each character is mapped to an integer. This output can then be decoded back into the character. 

In addition, he talks about different sorts of tokenization methods such as 'sentence piece' by Google and 'tick token' by OpenAI. Both these methods use sub-word level tokenization where not the whole words, but parts of words are converted into integers. This type of tokenization generally leads to shorter sequences of integers with larger vocabularies. Alternatively, one can have longer sequences with smaller vocabularies.