Here the speaker is explaining how language models, specifically a type of model called a Transformer, are designed to understand and predict the sequence of words or characters. 

To illustrate this, he uses the example of building a lighter model, using a compilation of all the works by Shakespeare. The system is trained by looking at a series of characters and attempting to predict the next character in the sequence. In essence, it's learning the patterns in Shakespeare's writing. Once the system is trained, it should be able to generate similar patterns.