ChatGPT is an artificial intelligence system developed by OpenAI that can generate text-based responses to prompts. In simple terms, if you give it a prompt such as "Write a note about Elon Musk buying Twitter," or "Write a breaking news article about a leaf falling from a tree," it generates a human-like written response.

The neural network which powers ChatGPT was proposed in a research paper called "Attention is all you need" from 2017. This neural network is called a Transformer and is designed to predict the next item in a sequence. So if you give it the beginning of a sentence, it can complete the sentence in a way that mimics human language.

ChatGPT generates responses not word by word, but by generating 'tokens' which are little chunks of words. In the background, there is a system that encodes these words into integers- a process known as tokenization. The encoding could be done at a character level, word level or sub-word level, depending on the tokenizer used. 

For example, you can use the 'tick token' library developed by OpenAI, which uses a 'byte pair encoding tokenizer'. Alternatively, Google uses the 'sentence piece' tokenizer, which encodes text into sub-word units.

In short, when a prompt is given to ChatGPT, it uses the transformer to predict the sequence of words or tokens that could possibly come next. Each of these words/token is encoded into a list of integers for processing.