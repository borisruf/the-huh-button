ChatGPT is a powerful artificial intelligence system that can generate text. It's a 'probabilistic system', meaning it can generate different responses to the same prompt. Under the hood, ChatGPT utilises Transformer architecture from a 2017 paper called 'Attention is all you need'. Transformer architecture is a type of neural network and has become an integral component in AI applications. 

To understand how this works, the speaker used a favourite data set, 'Tiny Shakespeare', to model and predict character sequences. After training, the system was able to generate Shakespeare-like language in a sequential manner. 

The speaker also mentioned a simple but efficient system for training Transformers called Nano GPT. Using Nano GPT, the speaker was able to reproduce parts of GPT2, an early version of OpenAI's GPT from 2017, verifying the code base's correctness and ability to load neural network weights.