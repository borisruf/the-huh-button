The author was discussing how the AI system, ChatGPT works. He explained that it's a language model that simply completes sequences of words or characters given to it in a prompt. He elaborated on the 'Transformer' architecture used in ChatGPT which he explained takes words from a prompt, and generates more words to complete the prompt. He gave examples of how ChatGPT could be used and then proposed to train a similar model based on a data set called 'Tiny Shakespeare'. This would mimic the workings of ChatGPT but on a smaller scale, predicting character sequences. He explained how the model learned from the data to generate 'Shakespeare-like' language. He had already written this code, which is available on his Nano GPT GitHub repository, and planned to recreate it in a Google Colab Jupyter notebook during a lecture. 
While explaining this, he said, "Here, I open the "input," probably referring to the start of his coding session where he would open the 'input' data file containing the 'Tiny Shakespeare' dataset to train the model.