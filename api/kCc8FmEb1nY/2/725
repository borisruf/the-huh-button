This section explains how ChatGPT, a text-based AI system, works and introduces the concept of language modeling. It highlights how an AI system like ChatGPT can generate different responses to the same input as it is probabilistic.

It provides an overview of the Transformer neural network architecture, the underlying system that powers ChatGPT and other sophisticated AI applications. The Transformer architecture was originally proposed in a paper titled 'Attention is All You Need' and has since greatly influenced the field of AI.

The discussion then turns to the task at hand - training a Transformer-based language model on a 'character-level' using a dataset known as Tiny Shakespeare. This model will attempt to predict which character is likely to follow a given sequence of characters and generate Shakespearean-like text.

Finally, it introduces tokenization, converting raw text into a sequence of integers according to a vocabulary. In the case of this language model, each character in the text is converted into an integer. Other more advanced models may use a 'sub-word level' where chunks of words rather than individual characters are converted into integers. Different libraries, such as Google's 'sentence piece' and OpenAI's 'tick token', offer these tokenization schemas.