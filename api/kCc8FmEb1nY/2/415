OpenAI developed a system called ChatGPT that interacts with people and carries out text-based tasks like writing poems or generating answers. This AI can produce different answers for the same prompts due to its probabilistic nature. It uses a neural network called a Transformer that models sequences of words and predicts what word could come next. 

I've managed to build a simple version of this system, where a Transformer-based language model is trained to predict the sequence of Shakespeare's language at a character level. Simply put, given a string of characters from Shakespeare, the system will try to predict the next character. I've written all the code for training these Transformers in a GitHub repository called Nano GPT. So far, I've reproduced the smallest model for GPT2 with 124 million parameters. This just verifies that the code and the neural network are correctly organized and I can load the weight of the neural network that OpenAI will be releasing in the future.
