The speaker starts by explaining what ChatGPT is, and the idea behind a language model. He mentions the use of transformer architecture "Attention is all you need" for ChatGPT. The speaker teaches how to build a simple transformer using Python. He works with the "Tiny Shakespeare" dataset which includes all works of Shakespeare in a single file. The characters are converted into integers for ease of processing. The speaker mentions there are different methods for encoding the text into integers, they choose character-level tokenizer for its simplicity. This provides them with very long sequences of simple encode and decode functions. An encoder and a decoder are used to tokenize the training set and these are then wrapped into a data tensor. This process helps to model how characters follow each other in a sequence.