Imagine you're playing a guessing game where you try to predict the next character in a sentence based on the characters you've seen so far. Let's say you've seen "The cat is chasin" and you have to guess the next character. The most likely guess would be 'g', making the sentence "The cat is chasing".

This is how the Transformer neural network in ChatGPT works. It's trained to predict the next character or word based on the previous ones in a sentence or piece of text. This AI model observes the patterns and structure in the way words follow each other in English and uses that knowledge to generate new, sensible sentences given a starting prompt. 

This model was first introduced in a research paper called "Attention is All You Need" in 2017 which showcased the Transformer architecture. The idea has been widely used in many AI applications, including OpenAI's ChatGPT.